# Default values for moondream-station.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: ghcr.io/mad-deecent/moondream-station-helm
  pullPolicy: Always
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

imagePullSecrets:
  []
  # - name: ghcr-secret  # Uncomment if using private registry
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext:
  fsGroup: 1000
  runAsNonRoot: true
  runAsUser: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: ClusterIP
  port: 8080
  targetPort: 2020

# LoadBalancer service for external access
externalService:
  enabled: false
  type: LoadBalancer
  port: 8080
  targetPort: 8080
  annotations: {}

ingress:
  enabled: false
  className: ""
  annotations:
    {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: moondream.local
      paths:
        - path: /
          pathType: Prefix
  tls: []
  #  - secretName: moondream-tls
  #    hosts:
  #      - moondream.local

resources:
  requests:
    cpu: 1000m
    memory: 2Gi
    nvidia.com/gpu: 1
  limits:
    cpu: 4000m
    memory: 8Gi
    nvidia.com/gpu: 1

# GPU configuration
gpu:
  enabled: true
  # Number of GPUs to request
  count: 1
  # GPU type constraint (optional)
  nodeSelector: {}
  # Tolerate GPU node taints
  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

# Persistent storage for models and data
persistence:
  enabled: true
  # Storage class name
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 10Gi
  # Existing PVC name (if you want to use an existing one)
  existingClaim: ""

# Shared memory for ML workloads
shm:
  enabled: true
  size: 2Gi

# Application configuration
config:
  host: "0.0.0.0"
  port: 2020
  logLevel: "info"
  maxConcurrentRequests: 10

# Environment variables
env:
  {}
  # CUSTOM_VAR: "value"

# Additional environment variables from ConfigMap/Secret
envFrom: []

# Probes configuration
probes:
  readiness:
    enabled: true
    path: /v1 # API endpoint for health check
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  liveness:
    enabled: true
    path: /v1 # API endpoint for health check
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 10
    failureThreshold: 3

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector:
  {}
  # Example GPU node selection (uncomment and modify as needed):
  # nvidia.com/gpu.memory: "12288"
  # nvidia.com/gpu.product: "NVIDIA-GeForce-RTX-3060"

tolerations: []

affinity: {}

# Pod disruption budget
podDisruptionBudget:
  enabled: false
  minAvailable: 1
  # maxUnavailable: 1
